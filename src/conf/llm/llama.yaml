llm:
  _target_    : src.models.models.HabitatModel
  _partial_   : True

# Put your host number here if you are using RLM
model_name   : "meta-llama/Llama-3.1-8B-Instruct"
url          : "http://165.132.142.55"
port          : 8008

verbose       : True


# The system message helps set the behavior of the assistant.
# "You are a helpful assistant." or "You're an expert in deep learning."
# system_message       : 'You are a helpful assistant.'
system_message       : 'You are an expert at task planning.'

# System, User, Assistant and end of turn tags
system_tag : "<|start_header_id|>system<|end_header_id|>\n\n"
user_tag : "<|start_header_id|>user<|end_header_id|>\n\n"
assistant_tag : "<|start_header_id|>assistant<|end_header_id|>\n\n"
eot_tag : "<|eot_id|>"

# Decide whether to keep history
keep_message_history   : False

generation_params:

  # The maximum number of tokens to generate in the completion.
  max_tokens    : 512

  # float , optional, defaults to 1.0) â€” The value used to modulate the next token probabilities.
  temperature   : 0.0

  # Returns the best `n` out of `best_of` completions made on server side
  n             : 1
  best_of       : 1

  # up to 4 sequences that stop the generation
  # stop          : '\n'
  stop          : "Assigned!"

  # Return a list of response or not
  batch_response: False

  # If we want to do sample method of text generation or not
  do_sample     : False

  # The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.
  repetition_penalty : 1.0

  # The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.
  top_k         : 50

  # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation. Default to 1.0
  top_p         : 1
