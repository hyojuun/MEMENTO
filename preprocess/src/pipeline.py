import json
from collections import defaultdict, Counter
from typing import List, Dict, Callable
import random
import itertools
from tqdm import tqdm

from utils import output_as_dataset, renew_episode_info, add_metadata, extract_original_info, add_stage_2
import pandas as pd


def check_used_object(df: pd.DataFrame, obj_id: str):
    return df[df['id'] == obj_id]

def check_used_object_category(df: pd.DataFrame, obj_id: str, category: str) -> bool:
    """
    just for object_category_df
    """
    filtered = df[(df['id'] == obj_id) & (df['clean_category'] == category)]
    return not filtered.empty 
    

def check_used_object_category_from_eval(eval: Dict, df: pd.DataFrame, keys: list):
    if eval['function_name'] in ["is_on_top", "is_inside", "is_in_room"]:
        handle = eval['args']['object_handles'][0]
        obj_id = handle.split(":")[0][:-1]
        target_df = check_used_object(df, obj_id)[keys]
        if not target_df.empty:
            if len(keys) == 1:
                target = {keys[0]: target_df.iloc[0][keys[0]]}  # ë‹¨ì¼ ê°’ ë°˜í™˜ as dict
            else:
                target = target_df.iloc[0].to_dict()  # ì—¬ëŸ¬ ê°œ ê°’ dict ë°˜í™˜
            return target
        else:
            return {}
    
    elif eval['function_name'] in ["is_next_to"]:
        # TODO: entity_a, entity_bëŠ” receptacleì´ ìˆëŠ” ê²½ìš°ë„ ìˆìŒ. ë”°ë¼ì„œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë¡œì§ í•„ìš”
        handle_a = eval['args']['entity_handles_a'][0]
        handle_b = eval['args']['entity_handles_b'][0]
        id_a = handle_a.split(":")[0][:-1]
        id_b = handle_b.split(":")[0][:-1]
        target_a_df = check_used_object(df, id_a)[keys]
        target_b_df = check_used_object(df, id_b)[keys]
        result = {}
        if not target_a_df.empty:
            if len(keys) == 1:
                result.update({keys[0]: target_a_df.iloc[0][keys[0]]})
            else:
                result.update(target_a_df.iloc[0].to_dict())
        if not target_b_df.empty:
            if len(keys) == 1:
                result.update({keys[0]: target_b_df.iloc[0][keys[0]]})
            else:
                result.update(target_b_df.iloc[0].to_dict())
        return result
    else:
        return {}
    
def check_used_handle(category: str, episode: list, df: pd.DataFrame) -> str:
    """
        categoryë¥¼ ë°›ì•„, dfì—ì„œ ì¡°íšŒí•´ í•´ë‹¹í•˜ëŠ” handleì„ ë°˜í™˜í•¨
    """
    target_objects = episode['target_objects']
    for obj in target_objects:
        # target_a_df = df[df['category'] == category][keys]
        pass
        
def remove_duplicates(data):
    unique_data = list({tuple(item.items()): item for item in data}.values())
    return unique_data
        

def edit_path(data_path: str):
    return "data" + data_path.split('data')[-1]


class TaskPipeline:
    def __init__(self, 
                 data_path: str,
                #  tuned_instructions: List[Dict], 
                 dataset: Dict, 
                 distractor_sampling_fn: Callable, 
                 object_category_df: pd.DataFrame,
                 category_cluster_df: pd.DataFrame,
                 sampled_dataset: Dict,
                 object_semantics_dataset = Dict,
                 object_description_dataset = Dict,
                 user_pattern_dataset = Dict
                 ):  
        """
        :param dataset: ì›ë³¸ instruction íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (List of Dict) /data/datasets/partnr_episodes/v0_0/train_2k.json
        :param tuned_instructions: íŠœë‹ëœ instruction ë¦¬ìŠ¤íŠ¸ (List of Dict)
        :param distractor_sampling_fn: ë°ì´í„° ìƒ˜í”Œë§ í•¨ìˆ˜
        """
        self.data_path = edit_path(data_path)
        
        self.dataset = dataset # raw datasets
        
        self.sampled_dataset = sampled_dataset
        ## About both
        # self.tuned_instructions = tuned_instructions # augmented instructions
        
        ## About object semantics
        self.distractor_sampling_fn = distractor_sampling_fn # sampling function (ì•„ì§ ë¯¸ì •) # => (ê·¸ëƒ¥ ì˜ˆì‹œ ì† ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
        self.object_category_df = object_category_df # objectì˜ category
        self.category_cluster_df = category_cluster_df # objectì˜ captionê³¼ cluster
        self.object_semantics_dataset = object_semantics_dataset
        self.object_description_dataset = object_description_dataset
        self.user_pattern_dataset = user_pattern_dataset
        
        self.final_dataset = {} # scene_id: [dataset_1, dataset_2, ...]

    def get_dataset_size(self):
        return len(self.dataset)
        
    def get_final_dataset_size(self):
        size = 0
        for k, datasets in self.final_dataset.items():
            for dataset in datasets:
                size += len(dataset)
        return size

    def pre_process(self):
        print(f"dataset size: {self.get_dataset_size()}")
        self.filter_by_scene()
        print(f"After filter_by_scene() -> dataset size: {self.get_dataset_size()}")
        self.preexclude_multi_eval()
        print(f"After preexclude_multi_eval() -> dataset size: {self.get_dataset_size()}")
        self.preexclude_multi_sampled()
        print(f"After preexclude_multi_sampled() -> dataset size: {self.get_dataset_size()}")
        self.preexclude_non_clusterd()
        print(f"After preexclude_non_clustered() -> dataset size: {self.get_dataset_size()}")
        self.prechange_handle()
        print(f"After prechange_handle() -> dataset size: {self.get_dataset_size()}")
        self.scenes = self.group_by_scene() # datas grouped by scenes
    
    def filter_by_scene(self):
        """
        sampled_datasetì—ëŠ” ì—†ëŠ” episodeê°€ ì¡´ì¬í•¨. ë”°ë¼ì„œ, ê·¸ê±¸ ë¯¸ë¦¬ filtering í•˜ëŠ” ê²ƒì´ ëª©ì 
        """
        sampled_episode_ids = list(self.sampled_dataset.keys())
        sampled_episode_ids_set = set(sampled_episode_ids)  # ë¦¬ìŠ¤íŠ¸ë¥¼ setìœ¼ë¡œ ë³€í™˜
        self.dataset = [data for data in self.dataset if data['episode_id'] in sampled_episode_ids_set]

    def preexclude_multi_eval(self):
        dataset = []
        for data in self.dataset:
            evals = data['evaluation_propositions']
            if any(len(eval['args'].get('object_handles', [])) >= 2 for eval in evals):
                continue  
            dataset.append(data)
        self.dataset = dataset
        
    def preexclude_multi_sampled(self):
        dataset = []
        for data in self.dataset:
            initial_states = data['info']['initial_state']
            if any(not state.get('name') and state.get('number') is not None and int(state.get('number')) > 1 
                   for state in initial_states):
                continue  
            dataset.append(data)  
        self.dataset = dataset
        
    def prechange_handle(self):
        """
        captionì´ ì—†ëŠ” handleì„ evalì— ì‚¬ìš©í•œë‹¤ë©´ ë¯¸ë¦¬ ë°”ê¿”ì£¼ëŠ” ê²ƒì´ ëª©ì 
        """
        dataset = []
        for i, episode in enumerate(self.dataset):
            episode_id = episode['episode_id'] #########################
             
            evals = episode['evaluation_propositions']
            modified_evals = []  # Store modified evaluations separately

            obj_info_all_list = []
            obj_info_caption_list = []

            # First, collect all obj_info
            for eval in evals:
                obj_info_all = check_used_object_category_from_eval(eval, self.object_category_df, ['id', 'clean_category'])
                obj_info_caption = check_used_object_category_from_eval(eval, self.category_cluster_df, ['id', 'caption', 'source', 'category', 'cluster'])
                if obj_info_all:
                    obj_info_all_list.append(obj_info_all)
                if obj_info_caption:
                    obj_info_caption_list.append(obj_info_caption)
            
            obj_info_all_list = remove_duplicates(obj_info_all_list)
            obj_info_caption_list = remove_duplicates(obj_info_caption_list)
            if episode_id == "1385":
                print(obj_info_all_list)
                print(obj_info_caption_list)

            # Handle cases where obj_info_caption_list is empty
            if not obj_info_caption_list:
                diff_objs = obj_info_all_list  # If no caption data exists, all obj_info_all are considered different
            else:
                diff_objs = [obj_info_all for obj_info_all in obj_info_all_list 
                                if obj_info_all['id'] not in {obj_info_caption['id'] for obj_info_caption in obj_info_caption_list}]
            
            if episode_id == "1385":
                print(diff_objs)

            for obj_info_all in diff_objs:
                if sampled_obj := self.sample_by_category(obj_info_all, 'clean_category'):
                    # obj_info_all ë¶€ë¶„ì„ sampled_objë¡œ ë³€ê²½í•´ì¤˜ì•¼ í•¨.
                    old_id = obj_info_all['id']
                    new_id = sampled_obj['id']

                    # rigid_objs ìˆ˜ì •
                    for obj in episode['rigid_objs']:
                        if old_id in obj[0]:
                            obj[0] = obj[0].replace(old_id, new_id)

                    # name_to_receptacle ìˆ˜ì •
                    updated_dict = {}
                    for k, v in episode['name_to_receptacle'].items():
                        new_k = k.replace(old_id, new_id) if old_id in k else k
                        updated_dict[new_k] = v  # ìƒˆë¡œìš´ keyë¡œ ê°’ ì €ì¥
                    episode['name_to_receptacle'] = updated_dict

                    # evaluation_propositions ìˆ˜ì •
                    for eval_item in evals:
                        modified_eval = eval_item.copy()
                        if eval_item['function_name'] in ['is_on_top', 'is_in_room', 'is_inside']:
                            if old_id in eval_item['args']['object_handles'][0]:
                                modified_eval['args']['object_handles'][0] = eval_item['args']['object_handles'][0].replace(old_id, new_id)
                        elif eval_item['function_name'] == "is_next_to":
                            if old_id in eval_item['args']['entity_handles_a']:
                                modified_eval['args']['entity_handles_a'][0] = eval_item['args']['entity_handles_a'][0].replace(old_id, new_id)
                            if old_id in eval_item['args']['entity_handles_b']:
                                modified_eval['args']['entity_handles_b'][0] = eval_item['args']['entity_handles_b'][0].replace(old_id, new_id)
                        modified_evals.append(modified_eval)  # Store modified eval
                    
            if diff_objs:
                episode['evaluation_propositions'] = modified_evals  # Update evaluations after iteration
                
            dataset.append(episode)

        self.dataset = dataset
        
    def preexclude_non_clusterd(self):
        dataset = []
        for i, episode in enumerate(self.dataset):
            used_object_categories = self.extract_categories_from_episode(episode)
            if not self.check_objects_can_sampled(used_object_categories):
                continue  # ìƒ˜í”Œë§í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
            else:
                dataset.append(episode)
        
        self.dataset = dataset

    def check_objects_can_sampled(self, used_object_categories):
        for category in used_object_categories:
            if category not in self.category_cluster_df['category'].values:
                return False    
        return True

    def group_by_scene(self) -> Dict[str, List[Dict]]:
        """
        Scene ID ê¸°ì¤€ìœ¼ë¡œ ì—í”¼ì†Œë“œ ê·¸ë£¹í™”
        :return: scene_idë¥¼ keyë¡œ í•˜ëŠ” episode dictionary ex. 203817140: [{}, {}]
        """
        scene_dict = defaultdict(list)
        for episode in self.dataset:
            scene_id = episode.get("scene_id", "unknown_scene")
            episode['used'] = False # ì‚¬ìš©ë˜ì—ˆëŠ”ì§€?
            scene_dict[scene_id].append(episode)
        return scene_dict

    def create_dataset(self):
        for _, (scene_id, episodes) in enumerate(tqdm(self.scenes.items())):
            self.final_dataset[scene_id] = self.create_dataset_per_scene(scene_id)
    
    def create_dataset_per_scene(self, scene_id: str):
        datasets = self.create_dataset_for_object(scene_id)
        datasets = self.create_dataset_for_user_pattern(scene_id, datasets)
        
        return datasets
    
    def check_objects_can_sampled(self, used_object_categories):
        for category in used_object_categories:
            if category not in self.category_cluster_df['category'].values:
                return False    
        return True

    
    def create_dataset_for_object(self, scene_id: str) -> List[Dict]:
        """
        Sampling per scene of original datasets.
        Each dataset contains 5 episodes.
        The process stops if the number of collected episodes exceeds half of the available episodes.
        The remaining half is used for the user pattern.
        """
        datasets = []
        episodes = self.scenes[scene_id]
        max_attempts = len(episodes) / 5  # ìµœëŒ€ ì‹œë„ íšŸìˆ˜
        attempt_count = 0

        while len(list(itertools.chain(*datasets))) < len(episodes) / 2 - 5:
            if attempt_count >= max_attempts:
                print(f"Max attempts reached. Stopping early for scene {scene_id}.")
                break  # Fail-safe: ì¼ì • íšŸìˆ˜ ì´ˆê³¼í•˜ë©´ ì¢…ë£Œ

            sampled_episodes = []
            indices_to_update = []  # 'used' ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•  ì¸ë±ìŠ¤ ì €ì¥
            total_used_object_categories = []

            episode_indices = list(range(len(self.scenes[scene_id])))            
            random.seed(42)
            random.shuffle(episode_indices)

            for i in episode_indices:
                if self.scenes[scene_id][i]['used']:
                    continue  # ì´ë¯¸ ì‚¬ìš©ëœ ì—í”¼ì†Œë“œëŠ” ìŠ¤í‚µ

                used_object_categories = self.extract_categories_from_episode(self.scenes[scene_id][i])
                sampled_episodes.append(self.scenes[scene_id][i])
                indices_to_update.append(i)
                total_used_object_categories.extend(used_object_categories)

                if len(sampled_episodes) == 5:  # 5ê°œê°€ ëª¨ì´ë©´ ì¢…ë£Œ
                    break

            # 5ê°œê°€ ëª¨ì¸ ê²½ìš°ì—ë§Œ datasets ì—…ë°ì´íŠ¸
            if len(sampled_episodes) == 5:
                datasets.append(sampled_episodes)
                for idx in indices_to_update:
                    self.scenes[scene_id][idx]['used'] = True  # ì‚¬ìš©ëœ ì—í”¼ì†Œë“œ í‘œì‹œ
            else:
                print(f"Skipping dataset update: Not enough valid episodes available.")

            attempt_count += 1  # ì‹œë„ íšŸìˆ˜ ì¦ê°€

        return datasets
        
    def create_dataset_for_user_pattern(self, scene_id: str, datasets: List):
        """
        Create dataset for user patterns. Ensures that only complete episode groups (size = 5) are added.
        If an incomplete dataset is created, it is removed from the list.
        """
        valid_datasets = []  # ìµœì¢…ì ìœ¼ë¡œ ìœ ì§€í•  datasets ë¦¬ìŠ¤íŠ¸

        for dataset in datasets:
            episode_cand = []
            indices_to_update = []  # 'used' ìƒíƒœë¥¼ ë³€ê²½í•  ì¸ë±ìŠ¤ ì €ì¥

            for j, episode in enumerate(self.scenes[scene_id]):
                if not self.scenes[scene_id][j]['used']:  # ì•„ì§ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì—í”¼ì†Œë“œë§Œ ì„ íƒ
                    episode_cand.append(episode)
                    indices_to_update.append(j)

                if len(episode_cand) == 5:  # 5ê°œê°€ ëª¨ì´ë©´ ì¤‘ë‹¨
                    break

            # 5ê°œë¥¼ ëª¨ì•˜ì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
            if len(episode_cand) == 5:
                dataset.extend(episode_cand)
                for idx in indices_to_update:
                    self.scenes[scene_id][idx]['used'] = True  # ì‚¬ìš©ëœ ì—í”¼ì†Œë“œ í‘œì‹œ
                valid_datasets.append(dataset)  # ìœ íš¨í•œ ë°ì´í„°ì…‹ë§Œ ì¶”ê°€
            else:
                print(f"Skipping dataset: Not enough unused episodes available.")

        return valid_datasets  # ìœ íš¨í•œ ë°ì´í„°ì…‹ë§Œ ë°˜í™˜

    def mid_process(self):
        # Step 1: extract_original_info ì ìš©
        updated_final_dataset = {}
        for scene_id, datasets in self.final_dataset.items():
            updated_datasets = [extract_original_info(dataset, self.data_path) for dataset in datasets]
            updated_final_dataset[scene_id] = updated_datasets
        self.final_dataset = updated_final_dataset

        # Step 2: renew_episode_info ì ìš©
        global_idx = 0
        updated_final_dataset = {}
        for scene_id, datasets in self.final_dataset.items():
            updated_datasets = []
            for dataset in datasets:
                updated_dataset = renew_episode_info(dataset, global_idx)
                global_idx += len(dataset)  # ì‚¬ìš©ëœ episode ê°œìˆ˜ë§Œí¼ global index ì¦ê°€
                updated_datasets.append(updated_dataset)
            updated_final_dataset[scene_id] = updated_datasets
        self.final_dataset = updated_final_dataset

        # Step 3: add_metadata ì ìš©
        updated_final_dataset = {}
        for scene_id, datasets in self.final_dataset.items():
            updated_datasets = [add_metadata(dataset) for dataset in datasets]
            updated_final_dataset[scene_id] = updated_datasets
        self.final_dataset = updated_final_dataset

        # Step 4: add_stage_2 ì ìš©
        updated_final_dataset = {}
        for scene_id, datasets in self.final_dataset.items():
            updated_datasets = [add_stage_2(dataset) for dataset in datasets]
            updated_final_dataset[scene_id] = updated_datasets
        self.final_dataset = updated_final_dataset

    
    def extract_categories_from_episode(self, episode):
        evals = episode['evaluation_propositions']
        used_categories = []
            
        for eval in evals:
            used_object = check_used_object_category_from_eval(eval, self.object_category_df, ['clean_category'])
            if (clean_category := used_object.get('clean_category')) is not None:
                used_categories.append(clean_category)
        
        return used_categories
        

    def sample_distractors(self):
        for scene_id, datasets in self.final_dataset.items():
            for i, dataset in enumerate(datasets):
                self.final_dataset[scene_id][i] = self.get_target_objects_per_dataset(dataset)
                self.final_dataset[scene_id][i] = self.sample_distractors_per_dataset(dataset)
    
    def sample_by_category(self, obj_info, key):
        category = obj_info[key]
        filtered_df = self.category_cluster_df[self.category_cluster_df["category"] == category]
        if not filtered_df.empty:
            random_row = filtered_df.sample(n=1)  # ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ì„ íƒ
            new_entry = random_row.to_dict(orient="records")[0]
            return new_entry
        else:
            return False
    
    def get_target_objects_per_dataset(self, dataset):
        """
        Datasetì—ì„œ ê° episodeì˜ target_objectsë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        """
        for episode in dataset:
            evals = episode['evaluation_propositions']
            object_infos = []

            for eval in evals:
                obj_info_caption = check_used_object_category_from_eval(eval, self.category_cluster_df, ['id', 'caption', 'source', 'category', 'cluster'])
                if obj_info_caption:
                    object_infos.append(obj_info_caption)

            # Counterë¥¼ í™œìš©í•˜ì—¬ ì¤‘ë³µ ì œê±° (í•œ ë²ˆë§Œ ë“±ì¥í•œ ê²ƒë§Œ ì„ íƒ)
            unique_object_infos = list({tuple(obj.items()): obj for obj in object_infos}.values())

            # Target objectsë¥¼ episodeì— ì¶”ê°€
            episode['metadata']['target_objects'] = unique_object_infos

        return dataset
    
    
    def sample_distractors_per_dataset(self, dataset):
        for i, episode in enumerate(dataset):
            if episode['metadata']['episode_type'] != "object_semantics": # user_pattern memorization only
                continue

            unique_object_infos = episode['metadata']['target_objects']
            distractors = []
            
            for obj in unique_object_infos:
                category = obj["category"]
                cluster = obj["cluster"]

                # ê°™ì€ category, ë‹¤ë¥¸ clusterì—ì„œ ìƒ˜í”Œë§
                filtered_df = self.category_cluster_df[
                    (self.category_cluster_df['category'] == category) &
                    (self.category_cluster_df['cluster'] != cluster)
                ]

                # ëœë¤ ìƒ˜í”Œë§ (í•´ë‹¹ ë°ì´í„°ê°€ ìˆì„ ê²½ìš°ì—ë§Œ)
                if not filtered_df.empty:
                    random_row = filtered_df.sample(n=1).iloc[0]  # ëœë¤ ìƒ˜í”Œë§
                    random_obj_dict = random_row.to_dict()
                    random_obj_dict.update({'pair_id': obj['id']})
                    distractors.append(random_obj_dict)  # ê²°ê³¼ ì €ì¥

            # Distractor ì •ë³´ë¥¼ ì›ë³¸ episodeì— ì¶”ê°€
            episode['metadata']['distractors'] = distractors

        return dataset  # ìˆ˜ì •ëœ dataset ë°˜í™˜
    
    def check_used_category_handle_from_episode(self, episode: dict, category: str):
        rigid_objs = episode['rigid_objs']
        targets = []
        for handle_config, trans in rigid_objs:
            handle = handle_config.split('.')[0]
            if check_used_object_category(df=self.object_category_df, obj_id=handle, category=category):
                targets.append(handle)

                try:
                    if episode['original_data_info']['episode_id'] == "1385":
                        print(handle)
                except Exception as e:
                    continue
                
        return targets
            
    def update_episodes(self, episodes: list): # final_datasetì˜ episodes
        for i, episode in enumerate(episodes):
            episode_id = episode['original_data_info']['episode_id']
                        
            if episode['metadata']['episode_type'] == 'object_semantics':
                object_semantics_data = self.object_semantics_dataset.get(episode_id, {})  # ê¸°ë³¸ê°’ì„ ë¹ˆ dictë¡œ ì„¤ì •
                
                # components for object_semantics
                object_semantic = object_semantics_data.get('stage1', "")
                object_semantic_stage2 = object_semantics_data.get('stage2', "")
                semantic_type = object_semantics_data.get('type', None)
                
                object_description = self.object_description_dataset.get(episode_id).get('description')
                
                if episode['metadata']['stage'] == "1":
                    tuned_instruction = episode['original_data_info']['instruction'] + object_description + object_semantic 
                    
                elif episode['metadata']['stage'] == "2":
                    tuned_instruction = object_semantic_stage2
                else:
                    raise Exception(f"episode_{episode_id}: episode['metadata']['stage'] errors")
                
                episode['metadata']['subtype'] = semantic_type
                episode['instruction'] = tuned_instruction
                
                # episode_idë¡œ sampleí•  ê²ƒ ê°€ì ¸ì˜¤ê¸° (used_objs) V
                used_object = object_semantics_data.get('used_object', [])
                sampled_episode = self.sampled_dataset[episode_id] # ì—¬ê¸°ì„œ handle position ê°€ì ¸ì™€ ì—…ë°ì´íŠ¸ í•  ê²ƒì„
                
                # used_objs ì† category ëŒ€ìƒìœ¼ë¡œ T / S ì–‘ìª½ì—ì„œ handle ì •ë³´ ì„ì‹œ ì €ì¥
                for obj_category in used_object:
                    if episode_id == "1385":
                        print(obj_category)
                        
                    targets = self.check_used_category_handle_from_episode(episode, obj_category)
                    sampled = self.check_used_category_handle_from_episode(sampled_episode, obj_category)
                    
                    # rigid_objs: Refactored transformation assignment
                    sampled_trans = []
                    for objs, trans in sampled_episode['rigid_objs']:
                        if objs.split('.')[0] in sampled:
                            sampled_trans.append(trans)

                    # Ensure there are enough transformations for both targets and distractors.
                    if len(sampled_trans) < 2 * len(targets):
                        raise ValueError("Not enough transformations for both targets and distractors.")

                    # Assign transformations: odd-indexed -> targets, even-indexed -> distractors
                    target_trans = [trans for i, trans in enumerate(sampled_trans) if i % 2 == 1]  # Odd-indexed
                    distractor_trans = [trans for i, trans in enumerate(sampled_trans) if i % 2 == 0]  # Even-indexed

                    # Assign target transformations
                    for i, target in enumerate(targets):
                        if i < len(target_trans):
                            for idx, (objs, origin_trans) in enumerate(episode['rigid_objs']):
                                if objs.split('.')[0] == target:
                                    episode['rigid_objs'][idx] = [objs, target_trans[i]]

                    # Assign distractor transformations
                    distractors = [
                        distractor for distractor in episode['metadata']['distractors']
                        if check_used_object_category(df=self.object_category_df, obj_id=distractor['id'], category=obj_category)
                    ]

                    for i, distractor in enumerate(distractors):
                        if i < len(distractor_trans):
                            episode['rigid_objs'].append([distractor['id'], distractor_trans[i]])
                        
                        
                    # name_to_receptacle
                    new_entries = {} 

                    for i, distractor in enumerate(distractors):
                        for k, v in episode['name_to_receptacle'].items(): 
                            if distractor.get('pair_id') in k:  
                                new_entries[distractor['id'] + "_:0000"] = v 

                    episode['name_to_receptacle'].update(new_entries)
                #   ì„ì‹œ ì •ë³´ í™œìš© ë³¸ê²© ë°”ê¾¸ê¸° -> ë‹¨ ì¡°ì‹¬í•´ì•¼ í•  ê²ƒì€, Tì˜ handleëª…ì€ ìœ ì§€í•˜ë˜, ì¢Œí‘œë§Œ Sì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨.
                #   ë”°ë¼ì„œ, Së¥¼ ë‘˜ë¡œ ë‚˜ëˆ”. a) sampleëœ ê²ƒ (info>extra_info>obj_info) b) ì›ë˜ ê²ƒ (ê±°ê¸° ì—†ì§€ë§Œ ì¹´í…Œê³ ë¦¬ ê°™ì€ ê²ƒ)
                
            
            elif episode['metadata']['episode_type'] == 'user_pattern':
                # components for user_pattern
                user_pattern_data = self.user_pattern_dataset.get(episode_id, {})
                user_pattern = user_pattern_data.get('stage1', "")
                user_pattern_stage2 = user_pattern_data.get('stage2', "")
                semantic_type = user_pattern_data.get('type', None)
                if episode['metadata']['stage'] == '1':
                    tuned_instruction = episode['original_data_info']['instruction'] + user_pattern
                elif episode['metadata']['stage'] == '2':
                    tuned_instruction = user_pattern_stage2
                else:
                    raise Exception(f"episode_{episode_id}: episode['metadata']['stage'] errors")
            
                episode['metadata']['subtype'] = semantic_type
                episode['instruction'] = tuned_instruction
            
            else:
                raise Exception(f"episode_{episode_id}: episode['metadata']['type'] errors")

                # let final_dataset (T) / sampled_dataset (S) V
                # T (original_data_info>episode_id)ì™€ S (info>extra_info>episode_id)ë¡œ ë§¤ì¹­ V
                # instruction ë¶ˆëŸ¬ì˜¤ê¸° V
                # tuned instruction ë¶ˆëŸ¬ì˜¤ê¸° (instruction: str = (inst+description+semantic), used_objs: list ([cup, tray])) V
            
                
    
    def change_handle(self, obj_category: str, episode: dict, sampled_episode: dict):
        # obj_categoryë¥¼ í†µí•´ Vë¡œ ë¶€í„° ê°€ì ¸ì˜¤ê³  Tì— ë°˜ì˜í•´ì•¼ í•¨
        #TODO: sampled_episodeë¡œë¶€í„° obj_categoryì— ë§ëŠ” ê°’ ê°€ì ¸ì˜¤ê¸°
        handle = check_used_handle(category=obj_category, episode=episode, df=self.category_cluster_df) # category_cluster_df ë„£ì€ ì´ìœ ëŠ” ì—¬ê¸° ë°–ì˜ ê²ƒì€ ì´ë¯¸ í•„í„°ë§ í•´ë†”ì„œ, ì—¬ê¸°ì„œ ì•ˆë˜ë©´ ì–´ì°¨í”¼ í„°ì§
        print(handle)
        #TODO: episodeì— í•´ë‹¹ ê°’ë“¤ ë°˜ì˜í•˜ê¸°
        
        #TODO: episodeì— distractor ë³€ê²½í•˜ê¸°
        
        pass


    def update_datasets(self):
        # self.final_dataset = {scene_id_1 = [[dataset_1], [dataset_2], ...]}
        for scene_id, datasets in self.final_dataset.items():
            for dataset in datasets:
                self.update_episodes(dataset)

    def change_instruction(self, episodes: list):
        for i, episode in enumerate(episodes):
            pass

    def update_instructions(self):
        for scene_id, datasets in self.final_dataset.items():
            pass
            
    def post_process(self):
        """
        1. 'used' í‚¤ ì œê±°
        2. ìµœì¢… habitat dataset í˜•íƒœ ë³€í™˜
        """
        for datasets in self.final_dataset.values():  # scene_idë¥¼ ì‚¬ìš©í•  í•„ìš” ì—†ìŒ
            for dataset in datasets:
                for episode in dataset:
                    episode.pop('used', None)  # í‚¤ê°€ ì—†ì„ ê²½ìš°ì—ë„ ì•ˆì „í•˜ê²Œ ì œê±°

        merged_dataset = [dataset for datasets in self.final_dataset.values() for dataset in datasets]
        self.final_dataset = merged_dataset
        

    def run(self) -> List[Dict]:
        """
        run the pipeline
        :return: final dataset
        """

        print("1. Filter not existing scenes, group by scene and change non-captioned handles...")
        self.pre_process()
        
        print("2. Sampling episodes per scene") # 2-1. Object semantics, 2-2. User pattern
        self.create_dataset() # self.final_dataset
        
        print("3. add metadata, original_info, process...")
        self.mid_process() # self.final_dataset

        print("4. sample distractors") # target, distractor sampling ì™„ë£Œ
        self.sample_distractors()
        
        print("5. update instruction") # LLM callí•´ì„œ description ì¶”ê°€í•˜ê¸° + ë¯¸ë¦¬ ë§Œë“¤ì–´ ë‘” semantics ì¶”ê°€í•˜ê¸°
        # self.update_instructions()
        
        print("6. update sample informations") # object sampling function í˜¹ì€ sampled dataset í™œìš©
        self.update_datasets()
        # 1) object_sampling_function -> 
        # 2) sampled_dataset -> í•´ë‹¹í•˜ëŠ” category ê°€ì ¸ì˜¤ê¸° => ê·¸ categoryì˜ rigid_objs listë§Œ ê°€ì ¸ì˜¤ê¸° 
        # => targetê³¼ distractorë¥¼ ê°ê° ëŒ€ì…
        # => name_to_receptacle, evaluation_propositionsì—” target ì—¬ì „íˆ ì‚¬ìš©
        print("7. postprocess")
        self.post_process()
        
        
        
        # df_tuple = self.preprocess_instruction_set()
        
        # print("ğŸ”¹ Tuned Instructionê³¼ Original Fileì„ ê²°í•©í•˜ì—¬ Dataset ìƒì„± ì¤‘...")
        # self.create_dataset(scenes, df_tuple)
        # print(f" ì´ {len(self.final_dataset)}ê°œì˜ ë°ì´í„° í•­ëª© ìƒì„± ì™„ë£Œ")
        # print("final", self.get_final_dataset_size())

        return self.final_dataset


def load_json(file_path: str) -> List[Dict]:
    with open(file_path, "r") as file:
        dataset = json.load(file)
    return dataset

def load_csv(file_path: str) -> pd.DataFrame:
    df = pd.read_csv(file_path)
    return df

def df_to_dict(df: pd.DataFrame, key: str) -> Dict:
    dict_file = df.groupby(key).apply(lambda x: x.drop(columns=[key]).to_dict(orient='records')).to_dict()
    return dict_file

def test(datasets: dict):
    # 1. ë„¤ ë²ˆ ì‚¬ìš©ëœ ì—í”¼ì†Œë“œ ìˆìœ¼ë©´ ì•ˆë¨
    episode_counts = Counter()

    for dataset in datasets:
        for data in dataset:
            episode_counts[data['episode_id']] += 1  # ê° episode_idì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ì¹´ìš´íŠ¸

    # 4ë²ˆ ì´ìƒ ë“±ì¥í•œ ì—í”¼ì†Œë“œ ì°¾ê¸°
    for episode_id, count in episode_counts.items():
        if count >= 4:
            print(f"âš  Warning: Episode {episode_id} is used {count} times!")

    # 2. object_handles > 2ì¸ ê²ƒì˜ instruction í™•ì¸í•´ë³´ì.
    for dataset in datasets:
        for i, data in enumerate(dataset):
            if i <= 4:
                for eval in data['evaluation_propositions']:
                    if len(eval['args'].get('object_handles', [])) >= 2:
                        print(data['instruction'])
                    
    


if __name__ == "__main__":
    # raw dataset
    split = "train_2k"
    
    dataset_path = f"/HabitatLLM/data/datasets/partnr_episodes/v0_0/{split}.json"
    dataset: list[Dict] = load_json(dataset_path)['episodes']
    
    category_cluster_df: pd.DataFrame = load_csv("/HabitatLLM/data/hssd-hab/metadata/category_cluster.csv")

    # object categories
    obj_category_path: str = "/HabitatLLM/data/hssd-hab/metadata/object_categories_filtered.csv"
    obj_category_df: pd.DataFrame = pd.read_csv(obj_category_path)
    
    sampled_path: str = f"/HabitatLLM/workspace/dataset/{split}/merged_episodes.json" # ì•ìœ¼ë¡œ ì´ í˜•ì‹ìœ¼ë¡œ í†µì¼
    sampled_dataset = load_json(sampled_path)
    
    object_semantics_path: str = f"/HabitatLLM/workspace/vllm_inference/res/{split}/object_semantics_tuned.json"
    object_semantics_dataset = load_json(object_semantics_path)
    
    object_description_path: str = f"/HabitatLLM/workspace/vllm_inference/res/{split}/object_description_tuned.json"
    object_description_dataset = load_json(object_description_path)
    
    user_pattern_path: str = f"/HabitatLLM/workspace/vllm_inference/res/{split}/user_pattern_tuned.json"
    user_pattern_dataset = load_json(user_pattern_path)
    
    # user_pattern_path: str = f"/HabitatLLM/workspace/vllm_inference/res/user_pattern/{split}/seed_try.json"
    # user_pattern_dataset = load_json(user_pattern_path)
    
    # ë”ë¯¸ ìƒ˜í”Œë§ í•¨ìˆ˜ ì •ì˜ (ì‹¤ì œ êµ¬í˜„ ì‹œ ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
    def dummy_distractor_sampling(data):
        data["augmented"] = True
        return data

    pipeline = TaskPipeline(data_path = dataset_path,
                            dataset = dataset, 
                            distractor_sampling_fn = dummy_distractor_sampling,
                            object_category_df = obj_category_df,
                            category_cluster_df = category_cluster_df,
                            sampled_dataset = sampled_dataset,
                            object_semantics_dataset = object_semantics_dataset,
                            object_description_dataset = object_description_dataset,
                            user_pattern_dataset = user_pattern_dataset)
    
    final_dataset = pipeline.run()

    test(final_dataset)
    
    output_path = f"/HabitatLLM/workspace/preprocess/dataset/final_dataset_{split}_test.json"
    final_dataset = output_as_dataset(final_dataset, output_path)

